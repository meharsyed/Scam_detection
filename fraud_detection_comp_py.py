# -*- coding: utf-8 -*-
"""fraud_detection_comp_notebook.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1u-gS53rXtaXcxzG7_XGOPnQtTT63B1yQ
"""

!pip install transformers

# Commented out IPython magic to ensure Python compatibility.

import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
from sklearn.model_selection import StratifiedShuffleSplit
# Required Packages and Libraries:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
# %matplotlib inline
import seaborn as sns

import re
import nltk
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer
from nltk.tokenize import sent_tokenize, word_tokenize
from sklearn.model_selection import train_test_split

import tensorflow as tf
from tensorflow import keras 
from tensorflow.keras.layers import Dense, Input
from tensorflow.keras.optimizers import Adam
from keras.models import Model

import transformers
from transformers import BertTokenizer, TFBertModel

from sklearn.metrics import confusion_matrix, accuracy_score, classification_report

import warnings
warnings.filterwarnings("ignore")

from google.colab import drive
drive.mount('/content/drive')

"""## Data Preprocessing"""

# Commented out IPython magic to ensure Python compatibility.
# %cd "/content/drive/MyDrive/Scam_detection_stuff/scam_dataset"

"""### Dataset 1: Phishing Dataset"""

df1 = pd.read_csv("phishing_templates_gpt1 - phishing_templates_gpt1.csv.csv", encoding ='latin-1', header = 0)
# Renaming the column names as:
new_headers = ['features', 'label']
df1.columns = new_headers

df1.head(5)

df1.info()

print(df1.shape)
df1.columns

# Checking unique values in phishing data
for col in df1.columns:
    unique_values = df1[col].nunique() 
    print(f"Column '{col}' has {unique_values} unique values")

# Replacing values in the 'label' column
df1['label'] = df1['label'].replace({0: 'real', 1: 'Fake'})

df1.head()

"""### Dataset 2: Scam(spam) Text Messages Dataset """

# Reading spam detection file as:
df2 = pd.read_csv("spam Text messages - spam Text messages.csv.csv")
df2.head(5)

# df2.columns

df2.describe()

# Checking the number of missing values in df2 columns:
df2_null = df2.isna().sum()
print(df2_null)
df2.info()

# Dropping the undesired (empty)columns as:

df2 = df2.drop(['Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4'], axis=1)

# Renamming the columns as features and label:
df2 = df2.rename(columns={'Is text Spam or not': 'label', 'Text': 'features'})


# Replacing values in the 'spam' column
df2['label'] = df2['label'].replace({'ham': 'Real', 'spam': 'Fake'})


# Reordering the columns
df2 = df2.reindex(columns=['features', 'label'])

print(df2.shape)
df2.head()

# Counting the real, fake entries in df2:

df2_fake = pd.DataFrame()
df2_real = pd.DataFrame()

df2_real = df2[df2['label'] == 'Real']

df2_fake = df2[df2['label'] == 'Fake']

# Print resulting dataframe
print(df2_fake.shape)
print(df2_real.shape)
df2_fake.head()

"""### Dataset 3: Real/ fake Job Posting Dataset"""

# Reading spam detection file as:
df3 = pd.read_csv("fake_job_postings - fake_job_postings.csv.csv")
df3.head(5)

# Exploring this dataset:
df3.info()

# Exploring the real/ fraud entries in this dataset
df3['fraudulent'].value_counts()
#data is very imbalanced

df3_text = df3.copy()
# Dropping undesired columns as:
df3_text.drop(columns=['job_id', 'telecommuting', 'has_company_logo', 'has_questions'], inplace=True)
df3_text.head(3)

# Combining Text attributes as:
# For job title attribute:
df3_text.title = df3_text['title'] + ' ' + df3_text['location']  + ' ' + df3_text['description']

# For job department attribute:
df3_text.department = df3_text['department'] + ' ' + df3_text['employment_type'] + ' ' + df3_text['required_experience'] + ' ' + df3_text['industry'] + ' ' + df3_text['function'] + ' ' + df3_text['required_education']

# Dropping the undesired columns:
df3_text.drop(columns=['location', 'description', 'employment_type', 'required_experience', 'industry', 'function', 'required_education'], inplace=True)

# Filling the NAN data
df3_text['title'].fillna('No Data', inplace=True)
df3_text['department'].fillna('No Data', inplace=True)
df3_text['salary_range'].fillna('No Data', inplace=True)
df3_text['benefits'].fillna('No Data', inplace=True)
df3_text['company_profile'].fillna('No Data', inplace=True)
df3_text['requirements'].fillna('No Data', inplace=True)

df3_text.head()

# Getting a new column with all the text fields as:

df3_text['full_text'] = df3_text['title'] + ' ' + df3_text['department']  + ' ' + df3_text['company_profile']  + ' ' + df3_text['requirements']  + ' ' + df3_text['benefits']
df3_text.columns
df3_text.head(3)

df3_text_up = pd.DataFrame()
df3_text_up['features'] = df3_text['full_text']
df3_text_up['label'] = df3_text['fraudulent']
df3_text_up.head()

# Replacing values in the 'label' column
df3_text_up['label'] = df3_text_up['label'].replace({0: 'Real', 1: 'Fake'})

df3_text_up.head()

"""### Dataset 4: Spam/ real emails Dataset """

# Saving all the spam/ ham statements in a CSV file with corresponding label:

import collections
import re
import random
import scipy.io
import glob
import csv
import sys
import codecs

# Dataset directories paths
spam_dir = "spam/"
ham_dir = "ham/"


def load_text_file(filenames):
    text_list = []
    for filename in filenames:
        with codecs.open(filename, "r", "utf-8", errors = 'ignore') as f:
            text = f.read().replace('\r\n', ' ')
            text_list.append(text)
    return text_list

# Getting fake(scam) sentences list
spam_filenames = glob.glob(spam_dir + '*.txt')
fake_list = load_text_file(spam_filenames)

# Getting Real Sentences list
ham_filenames = glob.glob(ham_dir + '*.txt')
real_list = load_text_file(ham_filenames)

print(len(fake_list))
print(len(real_list))

# Creating a dataframe by integrating these lists with correponding labels

# corresponding label lists
real_labels = ['Real'] * len(real_list)
fake_labels = ['Fake'] * len(fake_list)

# Combining the sentences and labels into a pandas DataFrame
sentences = real_list + fake_list
labels = real_labels + fake_labels
df4 = pd.DataFrame({'features': sentences, 'label': labels})

# Shuffle the rows in the DataFrame
df4 = df4.sample(frac=1).reset_index(drop=True)

df4.head()

"""### Dataset 5: Recruitment Scam Dataset"""

# Reading Recruitment scam data detection file as:
df5 = pd.read_csv("RecruitmentSCAM - RecruitmentSCAM.csv")
df5.head(5)

# Exploring the Recruitment dataset:
df5.info()

# Exploring the real/ fraud entries in this dataset
df5['fraudulent'].value_counts()
#data is very imbalanced

df5_text = df5.copy()
# Dropping undesired columns as:
df5_text.drop(columns=['telecommuting', 'has_company_logo', 'has_questions', 'in_balanced_dataset'], inplace=True)
df5_text.head(3)

# Filling the NAN data
df5_text['title'].fillna('No Data', inplace=True)
df5_text['department'].fillna('No Data', inplace=True)
df5_text['salary_range'].fillna('No Data', inplace=True)
df5_text['benefits'].fillna('No Data', inplace=True)
df5_text['company_profile'].fillna('No Data', inplace=True)
df5_text['requirements'].fillna('No Data', inplace=True)
df5_text.head()

# Combining Text features attributes as:

# For job title attribute:
df5_text.title = df5_text['title'] + ' ' + df5_text['location']  + ' ' + df5_text['description']

# For job department attribute:
df5_text.department = df5_text['department'] + ' ' + df5_text['employment_type'] + ' ' + df5_text['required_experience'] + ' ' + df5_text['industry'] + ' ' + df5_text['required_education']

# Dropping the undesired columns:
df5_text.drop(columns=['location', 'description', 'employment_type', 'required_experience', 'industry', 'function', 'required_education'], inplace=True)

# Getting a new column with all the text fields as:

df5_text['full_text'] = df5_text['title'] + ' ' + df5_text['department']  + ' ' + df5_text['company_profile']  + ' ' + df5_text['benefits']

df5_text.head()

df5_text = df5_text.dropna(subset=['full_text']).reset_index(drop=True)
df5_text.head()

df5_text_up = pd.DataFrame()
df5_text_up['features'] = df5_text['full_text']

df5_text_up['label'] = df5_text['fraudulent']
df5_text_up.head()

# Replacing values in the 'label' column
df5_text_up['label'] = df5_text_up['label'].replace({'f': 'Real', 't': 'Fake'})
print(df5_text_up.shape[0])
df5_text_up.head()

df5_text_up.info()

"""### Concatenating the Datasets"""

# Concatenate all the dataframes and shuffleing the rows

df_conc = pd.concat([df1, df2, df3_text_up, df4, df5_text_up]).sample(frac=1).reset_index(drop=True)

print(df_conc.shape)
df_conc.head()

# Converting the labels with 1 and 0 as:
# Real = 0
# Fake(Scam) = 1

df_conc['label'] = df_conc['label'].map({'Real':0, 'Fake':1})
df_conc.head()

"""### Exploratory Data Analysis and Visualization"""

# only for verification purpose:

# Counting the real, fake entries in df_conc:

df_conc_fake = pd.DataFrame()
df_conc_real = pd.DataFrame()

df_conc_real = df_conc[df_conc['label'] == 0]

df_conc_fake = df_conc[df_conc['label'] == 1]

# Print resulting dataframe
print(df_conc_fake.shape[0])
print(df_conc_real.shape[0])
sum_real_fake  = df_conc_fake.shape[0] + df_conc_real.shape[0]
print('Sum of real and Fake Entries in conc DF :',sum_real_fake)
print('Total no of rows in Concatenated dataframe :',df_conc.shape[0],'\n\n')
df_conc_fake.head()

# Verifying the number of Real/ fake(fraud) text messages(statements)
df_conc['label'].value_counts()

# checking percentage
sms = pd.value_counts(df_conc["label"], sort=True)
sms.plot(kind="pie", labels=["Real(0)", "Fake/fraud(1)"], autopct="%1.0f%%")

plt.title("Text messages Distribution")
plt.ylabel("")
plt.show()

sns.set(style = "ticks" , font_scale = 1.0)
sns.countplot(df_conc.label).set_title("Number of Real and Fake(scam) messages")
plt.show()

# For Visualization of lengths of Text Messages:
df_conc['length'] = df_conc.features.apply(len)
df_conc.head()

plt.figure(figsize=(8, 5))
df_conc[df_conc.label == 0].length.plot(bins=50, kind='hist', color='Darkgreen', label='Real', alpha=0.6)
df_conc[df_conc.label == 1].length.plot(kind='hist', color='red', label='Fake/fraud', alpha=0.6)
plt.legend()
plt.xlabel("Messages Length");

"""## Pre-processing on Text Features Data"""

import nltk

# Downloading and exporting stopwords list and Stemmer
nltk.download('stopwords')
stop_words = nltk.corpus.stopwords.words('english')
print(stop_words[::10])

porter = nltk.stem.PorterStemmer()

# Defining feature text processing functions
def clean_text(words):

    """The function to clean text"""

    words = re.sub("[^a-zA-Z]"," ", words)
    text = words.lower().split()                   
    return " ".join(text)

def remove_stopwords(text):
    """The function to removing stopwords"""

    text = [word.lower() for word in text.split() if word.lower() not in stop_words]
    return " ".join(text)

def stemmer(stem_text):
    """The function to apply stemming"""
    
    stem_text = [porter.stem(word) for word in stem_text.split()]
    return " ".join(stem_text)

# Applying these operations on the 'features' column of the Dataset:

# Calling the Clean_text function as:
df_conc['features'] = df_conc['features'].apply(clean_text)

# Removing stop words from text as:
df_conc['features'] = df_conc['features'].apply(remove_stopwords)


# Applying Stemming on text as:
df_conc['features'] = df_conc['features'].apply(stemmer)

# updating lenths column and visualizing the differnce as:
df_conc['length_postProcess'] = df_conc.features.apply(len)

df_conc.head(10)

print(df_conc.columns)
# Dropping the 'length' and 'length_postProcess' columns:

df = df_conc
df = df.drop(['length', 'length_postProcess'], axis=1)
df.head()

"""## Balanace Splitting the Dataset"""

# Separating features and labels columns as:
X = df['features']
y = df['label']

# Creating a StratifiedShuffleSplit object to split the data into train and test sets
sss = StratifiedShuffleSplit(n_splits = 1, test_size = 0.15, random_state = 50)

# Spliting the data into train and test sets
for train_index, test_index in sss.split(X, y):
    X_train, X_test = X.iloc[train_index], X.iloc[test_index]
    y_train, y_test = y.iloc[train_index], y.iloc[test_index]

# Creating another StratifiedShuffleSplit object to split the train set into train and validation sets
sss2 = StratifiedShuffleSplit(n_splits = 1, test_size = 0.2, random_state = 50)

# Split the train set into train and validation sets
for train_index, val_index in sss2.split(X_train, y_train):
    X_train, X_val = X_train.iloc[train_index], X_train.iloc[val_index]
    y_train, y_val = y_train.iloc[train_index], y_train.iloc[val_index]

# Print the number of samples in each split
print("Train set samples: ", len(X_train))
print("Validation set samples: ", len(X_val))
print("Test set samples: ", len(X_test))

# Print the number of sample labels in each split
print("Train set labels: ", len(y_train))
print("Validation set labels: ", len(y_val))
print("Test set labels: ", len(y_test))

print(X_train.values)

"""## Implementing BERT Model for Spam/ Fraud Detection"""

# Load the tokenizer
tokenizer = BertTokenizer.from_pretrained('bert-large-uncased')

tokenizer

# Load the pre-trained BERT model
bert_model = TFBertModel.from_pretrained('bert-base-uncased')

def encode(text, maxlen):
  input_ids=[]
  attention_masks=[]

  for row in text:
    encoded = tokenizer.encode_plus(
        row,
        add_special_tokens=True,
        max_length=maxlen,
        pad_to_max_length=True,
        return_attention_mask=True,
    )
    input_ids.append(encoded['input_ids'])
    attention_masks.append(encoded['attention_mask'])

  return np.array(input_ids),np.array(attention_masks)

# Encoding our testing, validation and training datasets as: 

X_train_input_ids, X_train_attention_masks = encode(X_train.values, maxlen=128)

X_val_input_ids, X_val_attention_masks = encode(X_val.values, maxlen=128)

X_test_input_ids, X_test_attention_masks = encode(X_test.values, maxlen=128)

"""## BERT customized Model

we are using BERT model with two dense layers and a softmax layer on top of it as:
"""

def build_model(bert_model):
   input_word_ids = tf.keras.Input(shape=(128,),dtype='int32')
   attention_masks = tf.keras.Input(shape=(128,),dtype='int32')

   sequence_output = bert_model([input_word_ids,attention_masks])
   output = sequence_output[1]
   output = tf.keras.layers.Dense(64,activation='relu')(output)
   output = tf.keras.layers.Dropout(0.2)(output)
   output = tf.keras.layers.Dense(1,activation='sigmoid')(output)

   model = tf.keras.models.Model(inputs = [input_word_ids,attention_masks], outputs = output)
   model.compile(Adam(learning_rate=1e-5), 
                 loss='binary_crossentropy',
                 metrics=['accuracy', tf.keras.metrics.Precision(), tf.keras.metrics.Recall(), tf.keras.metrics.AUC()]
                 )

   return model

# Building Model and Visualizing its summary as:
model = build_model(bert_model)
model.summary()

"""### Training the model """

!pip install tensorboard

import tensorflow as tf
from tensorflow.keras.callbacks import TensorBoard


# Define the TensorBoard callback to visualize the training
tensorboard_callback = TensorBoard(log_dir='/content/logs', histogram_freq=1)

# To reduce the imbalance class label effect, we can set its weights to six times:
class_weight = {0: 1, 1: 1}
     
# Training the model as:
history = model.fit(
    [X_train_input_ids, X_train_attention_masks],
    y_train,
    batch_size = 32,
    epochs=5,
    callbacks=[tensorboard_callback],
    validation_data=([X_val_input_ids, X_val_attention_masks], y_val),
    class_weight=class_weight
    )

# Commented out IPython magic to ensure Python compatibility.

# Load TensorBoard in a notebook cell
# %load_ext tensorboard
# %tensorboard --logdir /content/logs

# Visualizing the training and validation error as:

def plot_graphs(history, string):
  plt.plot(history.history[string])
  plt.plot(history.history['val_'+string])
  plt.xlabel("Epochs")
  plt.ylabel(string)
  plt.legend([string, 'val_'+string])
  plt.show()
  
plot_graphs(history, "accuracy")
plot_graphs(history, "loss")

# Save the model to drive directory
model.save('scam_detection_model.h5')

import transformers
# Load the saved model
loaded_model = tf.keras.models.load_model('scam_detection_model.h5', custom_objects={"TFBertModel": transformers.TFBertModel})

"""## Inference of the trained model"""

# Evaluating the model on Test Data
loss, accuracy, precision, recall, auc  = loaded_model.evaluate([X_test_input_ids, X_test_attention_masks], y_test)
print('Test accuracy of the model :', accuracy)
print('\n',"Model Loss:", loss)

y_predicted = loaded_model.predict([X_test_input_ids, X_test_attention_masks])
y_predicted = y_predicted.flatten()
     
import numpy as np

y_predicted = np.where(y_predicted > 0.5, 1, 0)
y_predicted

from sklearn.metrics import confusion_matrix, classification_report

cm = confusion_matrix(y_test, y_predicted)
cm

from matplotlib import pyplot as plt
import seaborn as sn
sn.heatmap(cm, annot=True, fmt='d')
plt.xlabel('Predicted')
plt.ylabel('Truth')

print(classification_report(y_test, y_predicted))

"""# Inference Block on Unseen samples


Note: all you need is to provide the unseen dataset file(.csv file) and do the following changes in the code as:


*   Load the trained model as shown
*   Change the unseen inference dataset file path in a variable 'inf_file_path'
*   Rename the title of text input data column as 'features' (in line 55) of inference_func().
*   Rename the title of lbaels column as 'label' in the same line

After successfully changes these lines, you can run these blocks and evaluate the model on any unseen dataset.



"""

inf_file_path = "testing_data_scam_classifier.csv"
trained_model = loaded_model

import numpy as np
import tensorflow as tf
from matplotlib import pyplot as plt
import seaborn as sn
from sklearn.metrics import accuracy_score, precision_score
from sklearn.metrics import confusion_matrix, classification_report
from IPython.display import display


def evaluate_model(loaded_model, X_inf_input_ids, X_inf_attention_masks, y_inf):
    # Load the saved model
    model = loaded_model

    # Use the model for inference
    y_pred = model.predict([X_inf_input_ids, X_inf_attention_masks])
    y_pred = y_pred.flatten()

    y_pred = np.where(y_pred > 0.5, 1, 0)      


    # # Convert the predicted probabilities to binary labels
    # # y_pred = np.argmax(y_pred, axis=1)
    # y_pred = y_pred.flatten()

    cm = confusion_matrix(y_inf, y_pred)

    sn.heatmap(cm, annot=True, fmt='d')
    plt.xlabel('Predicted labels')
    plt.ylabel('Truth labels')

    print(classification_report(y_inf, y_pred))


    # Calculate the accuracy and precision of the model
    accuracy = accuracy_score(y_inf, y_pred)
    precision = precision_score(y_inf, y_pred)

    return accuracy, precision


def inference_func(inf_file_path, loaded_model):

    
    # Inference dataset with corresponding labels:
    df_temp = pd.read_csv(inf_file_path)

    # Shuffle the rows randomly
    df_temp = df_temp.sample(frac=1).reset_index(drop=True)

    test_df = df_temp.copy()
    # Display the new test DataFrame
    # display(test_df.head(10))

    # Renamming the columns as features and label:
    test_df = test_df.rename(columns={'testing_data': 'features', 'labels': 'label'})

    # Converting the labels with 1 and 0 as:
    # Real = 0
    # Fake(Scam) = 1

    test_df['label'] = test_df['label'].map({'real':0, 'scam':1})

    # Applying pre_processing operations on the 'features' column of the Dataset:

    # Calling the Clean_text function as:
    test_df['features'] = test_df['features'].apply(clean_text)

    # Removing stop words from text as:
    test_df['features'] = test_df['features'].apply(remove_stopwords)


    # Applying Stemming on text as:
    test_df['features'] = test_df['features'].apply(stemmer)

    # The inference data will be 
    X_inf = test_df['features']
    y_inf = test_df['label']

    # Calling the Encoder function to get input_ids and attention_masks
    X_inf_input_ids, X_inf_attention_masks = encode(X_inf.values, maxlen=128)
    

    # Calling the model for inference as:
    acc, prec = evaluate_model(loaded_model, X_inf_input_ids, X_inf_attention_masks, y_inf)

    print("Accuracy = ",acc)
    print("precision",prec)

# Calling the main inference function

inference_func(inf_file_path, trained_model)

